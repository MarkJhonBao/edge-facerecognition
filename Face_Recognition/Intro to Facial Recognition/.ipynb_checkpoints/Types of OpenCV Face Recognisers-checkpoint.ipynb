{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background on Face Recognition\n",
    "---\n",
    "How can a computer recognise a face?\n",
    "\n",
    "Taking a real life example: when you meet someone for the first time, you don't know who that person is, you gather data as you look at them and your brain gets 'trained' on those features.\n",
    "\n",
    "When you learn their name, for example Zendaya, now you know that the data belongs to Zendaya.\n",
    "\n",
    "The next time you see her or a picture of her face, your mind will follow this process:\n",
    "\n",
    "* **Face Detection**: look at the picture and find a face in it.\n",
    "* **Data Gathering**: extract unique characteristics of Zendaya's face that can be used to differetiate her from another person (eyes, mouth, nose, etc).\n",
    "* **Data Comparison**: despite variations in light or expression, it will compare those unique features to all the features of all the people you know.\n",
    "* **Face Recognition**: it will determine if Zendaya is actually Zendaya.\n",
    "\n",
    "Then, the more you meet Zendaya, the more data is going to be collected. Therefore, your mind is going to be quicker to recognise her. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory of OpenCV Face Recognisers\n",
    "---\n",
    "There are three steps to code facial recognition:\n",
    "* **Data Gathering**: gather face data (face images for example) of the persons you want to identify.\n",
    "* **Train the Recogniser**: feed that face data and respective names of each face to the recogniser so it can learn.\n",
    "* **Recognition**: feed new faces of that people and see if the face recogniser you just trained recognises them.\n",
    "---\n",
    "**OpenCV** has three built-in face recognisers:\n",
    "* EigenFaces - cv2.face.createEigenFaceRecognizer()\n",
    "* FisherFaces - cv2.face.createFisherFaceRecognizer()\n",
    "* Local Binary Patterns Histograms (LBPH) - cv2.face.createLBPHFaceRecognizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EigenFaces face recogniser\n",
    "---\n",
    "This algorithm considers the fact that **not all parts of a face are equally important or useful for face recognition**.\n",
    "\n",
    "In that sense, you are focusing on the areas of maximum change. For example, from the eye to the nose there is a significant change. When you look at multiple faces, you compare them by looking at these areas, because by catching the maximum variation among faces, they help you differentiate one face from the other.\n",
    "\n",
    "This is how EigenFaces work. It looks at all the training images of all the people as a whole and tries to extract the components which are relevant and useful and discards the rest. These important features are called **principal components**.\n",
    "\n",
    "Variance extracted from a list of faces:\n",
    "![Bunch of faces!](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/54_blog_image_6.jpg \"Bunch of faces!\")\n",
    "Source: http://docs.opencv.org/\n",
    "\n",
    "So, EigenFaces recogniser trains itself by extracting principal components, but it also keeps a record of which ones belong to which person. Thus, whenever you introduce a new image to the algorithm, it repeats the same process as follows:\n",
    "1. Extract the principal components from the new picture.\n",
    "2. Compare those features with the list of elements stored during training.\n",
    "3. Find the ones with the best match.\n",
    "4. Return the ‘person’ label associated with that best match component.\n",
    "\n",
    "**However, one thing to note in above image is that EigenFaces algorithm also considers illumination as an important feature. In consequence, lights and shadows are picked up by EigenFaces, which classifies them as representing a ‘face.'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FisherFaces face recogniser\n",
    "---\n",
    "This algorithm is an improved version of the last one. It extracts principal components that differentiate one person from the others. In that sense, an individual's components to not dominate (become more useful) over the others.\n",
    "\n",
    "Below is an image of principal components using FisherFaces algorithm:\n",
    "![Faces again!](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/54_blog_image_9.jpg \"Faces again!\")\n",
    "Source: http://docs.opencv.org/\n",
    "\n",
    "One thing to note is that FisherFaces only prevents features of one person from becoming dominant, but it still considers illumination changes as a useful feature. We know that light variation is not a useful feature to extract as it is not part of the actual face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local binary patterns histograms (LBPH) Face Recogniser\n",
    "---\n",
    "We know that Eigenfaces and Fisherfaces are both affected by light and, in real life, we can't guarantee perfect light conditions. LBPH face recognizer is an improvement to overcome this drawback.\n",
    "\n",
    "The idea with **LBPH** is not to look at the image as a whole, but instead, try to find its local structure by comparing each pixel to the neighboring pixels. \n",
    "\n",
    "### The LBPH Face Recognizer Process\n",
    "Take a 3×3 window and move it across one image. At each move (each local part of the picture), compare the pixel at the center, with its surrounding pixels. Denote the neighbors with intensity value less than or equal to the center pixel by 1 and the rest by 0.\n",
    "\n",
    "After you read these 0/1 values under the 3×3 window in a clockwise order, you will have a binary pattern like 11100011 that is local to a particular area of the picture. When you finish doing this on the whole image, you will have a list of **local binary patterns**.\n",
    "\n",
    "![](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/54_blog_image_10.jpg \"Local Binary Patterns\")\n",
    "Source: López & Ruiz; Local Binary Patterns applied to Face Detection and Recognition.\n",
    "\n",
    "Now, after you get a list of local binary patterns, you convert each one into a decimal number using binary to decimal conversion (as shown in above image) and then you make a histogram of all of those decimal values. A sample histogram looks like this:\n",
    "![](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/54_blog_image_11.jpg)\n",
    "\n",
    "In the end, you will have one histogram for each face in the training data set. That means that if there were 100 images in the training data set then LBPH will extract 100 histograms after training and store them for later recognition. Remember, the algorithm also keeps track of which histogram belongs to which person.\n",
    "\n",
    "Later during recognition, the process is as follows:\n",
    "\n",
    "1. Feed a new image to the recognizer for face recognition.\n",
    "2. The recognizer generates a histogram for that new picture.\n",
    "3. It then compares that histogram with the histograms it already has.\n",
    "4. Finally, it finds the best match and returns the person label associated with that best match.\n",
    "\n",
    "Below is a group of faces and their respective local binary patterns images. You can see that the **LBP faces are not affected by changes in light conditions**:\n",
    "[](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/54_blog_image_12.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
